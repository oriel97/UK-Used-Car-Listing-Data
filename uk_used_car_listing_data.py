# -*- coding: utf-8 -*-
"""UK Used Car Listing Data.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EI-nigvZYLatVDguYBD6m_UGpAqT1rca

## ***UK Used Car Listing Data*** ðŸš•
Oriel Asaraf
ID 313453540

The aim of this project is to forecast the price of a car. To achieved this goal, I found a dataset from Kaggle published in 2022 which contain more than 818,000 rows of records.
Each row represents one vehicle with more than 30 columns each represents a attribute oif the advert, such as the price, car make, model, variant, transmission, engine size, body type etc..
The main challenges in this project was the nature of the data. First, the data contains a lot of categorical features which are complicated to use in state-of-the-art models. Secondly, the  the target (price) and the other numerical features are not meeting normal distribution.
To handled the categorical features, a deep exploration of the nature of the features has been applied, including handling missing data, remove categorical data with high number of uniques values, remove features with high correlation, codding etc.
To handled the anormal distribution, I try to remove outliers and see if the distribution has improved. The optimal multiplier has been found and use.
After pre-processing, model selection, fine tuning, and exploration of feature importance I found an accuracy of 97% of the forecasting.

As part of this project, many experiments were conducted to improve the prediction of the target column, but I did not mention them to avoid overloading the notebook.
"""

import os
import kagglehub
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
from matplotlib.patches import Patch
from sklearn.preprocessing import StandardScaler, OrdinalEncoder
from sklearn.model_selection import cross_val_score, KFold
from sklearn.pipeline import Pipeline
from sklearn.metrics import mean_squared_error, accuracy_score, f1_score, make_scorer
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from xgboost import XGBRegressor
import xgboost as XGB
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, mean_absolute_percentage_error
from sklearn.model_selection import GridSearchCV
from scipy.stats import skew, kurtosis
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import shap
# Suppress all warnings
warnings.filterwarnings("ignore")

# Download latest version
path = kagglehub.dataset_download("guanhaopeng/uk-used-car-market")

print("Path to dataset files:", path)

csv_file_path = f"{path}/all_car_adverts.csv"
df = pd.read_csv(csv_file_path)

SKIP_FINDING_HYPER_PARAMTER = True
SKIP_FINFING_BEST_MODEL = True
SKIP_RUN_EXPIRIMENT = True

"""# **Part 1: Data Exploration and Pre-processing**

At the start, let's understand our data. The dataset contains 818,456 rows and 32 columns. The columns include binary, categorical, and numeric types, but some of them are not needed.
"""

df

df.describe()

"""As we can see, the data is distributed across a wide range of values. The difference between the minimum and maximum values of the target feature is very high.

Let's explore the data!

First, I want to show how the distribution looks for the car price feature:
"""

plt.figure(figsize=(7, 3))
sns.histplot(df['car_price'], bins=1000, kde=True)
sns.kdeplot(df['car_price'], fill = True)
plt.xlabel('Car price')
plt.ylabel('Density')
plt.title('Distribution of car_price')
plt.xlim([-10000,300000])
plt.show()

"""As I can see, the car price distribution does not follow a normal distribution.

The graph has a right skew, which means there are a small number of car prices that are significantly higher than the mean.

Many models and approaches assume a normal distribution, making this a challenge in my project. To address this, I will normalize the data to ensure a more normal distribution for better predictions.

As I explained, my data is built from three types of features: categorical, binary, and numeric.

I will explore each of them separately.

## **Pre** **processing**

First, I want to check the percentage of null values in each column.

I cannot have null values in my rows, as this will affect the results of my model.
"""

missing_percentages = (df.isnull().sum() / len(df)) * 100
missing_percentages

"""**Missing Data:**
To handle missing data, I will remove columns with a high percentage (>15%) of missing values, as they are not informative enough. For the remaining missing values, I will handle them differently based on the type of columnâ€”categorical or numerical.

Before deleting all the columns above the threshold, I want to save the num_owner column separately. I plan to conduct an experiment with this data later.

The number of owners (num_owner) of a car is critical in determining the car price. As the number of owners increases, the price tends to decrease.
"""

num_owners_saved_df = df[['num_owner']].copy()

# high percentage of missing values
threshold = 0.15  # Set threshold for missing values
df = df.loc[:, df.isnull().mean() < threshold]
missing_percentages = (df.isnull().sum() / len(df)) * 100
missing_percentages

"""# Handling Categorical Features
**After deleting the non-informative columns with too many missing values, I want to start processing the categorical features.**

First, I will understand the nature and distribution of the categorical features.

Our categorical features are:
"""

categorical_columns = df.select_dtypes(include=['object', 'category']).columns.tolist()
categorical_columns

""""Unnamed" is also a categorical feature. It appears to be numeric because all the values are numbers, but it is actually just the IDs of the rows. I will drop this feature."""

# Remove unnecessary columns i.e., 'Unnamed: 0'
df = df.drop(columns = 'Unnamed: 0' )

"""Let's examine our categorical data, and then I will handle it:"""

def show_uniques_graph(categorical_col):
  plt.figure(figsize=(10, 6))

  sns.barplot(
      x=categorical_col,
      y=[df[col].nunique() for col in categorical_col],
      palette='viridis'
  )

  plt.yscale('log')
  plt.xlabel("Features")
  plt.ylabel("Distinct Value Count")
  plt.title("Distinct Value Counts for Given Features")
  plt.xticks(rotation=45)
  plt.tight_layout()

  for p in plt.gca().patches:
      height = p.get_height()
      plt.text(p.get_x() + p.get_width() / 2, height, f'{int(height)}', ha='center', va='bottom', fontsize=10)

  plt.gca().set_yticklabels([])
  plt.gca().tick_params(axis='y', left=False)

  plt.show()
show_uniques_graph(categorical_columns)

"""This graph shows us the distinct value counts for each categorical featureâ€”the amount of unique values for each feature.

After some research, I've identified that there are columns which are not relevant at all to car price prediction, such as:
**car seller, car seller location, car subtitle, car attention grabber, car badges, car spec.**
These columns contain free text, and it will be very difficult for the model to learn from them. They seem very irrelevant, so I will drop them.

**Car title**: The car title feature is important, but it's also a free text feature. This feature combines the make and model, so I will save those columns separately, making this feature unnecessary as well.

**Variant**: After checking, I found that this column doesn't affect the price significantly and has too many unique values. Therefore, I will drop it as well.

"""

df = df.drop(columns=['car_specs', 'car_sub_title', 'car_attention_grabber', 'car_seller', 'car_seller_location','variant','car_title', 'car_badges'])

"""The dataset contains a column named "engine_size_unit." Upon reviewing the data, it is evident that engine size is described by two columns: **engine_size_unit** and **engine_size**. The engine size is measured in either PS or BHP units, with the majority of entries using PS as the unit of measurement. Therefore, I will convert the engine size values currently measured in BHP to PS and subsequently remove the "engine_size_unit" column."""

df.loc[df['engine_size_unit'] == 'bhp', 'engine_size'] *= 1.01442
df = df.drop(columns=['engine_size_unit'])

categorical_columns = df.select_dtypes(include=['object', 'category']).columns.tolist()
show_uniques_graph(categorical_columns)

!pip install thefuzz
from thefuzz import fuzz, process

def get_fuzzy_unique_count(series, threshold=85):
    unique_values = list(series.unique())
    grouped_values = []

    for value in unique_values:
        matched = False
        for group in grouped_values:
            if fuzz.ratio(value, group[0]) >= threshold:
                group.append(value)
                matched = True
                break

        if not matched:
            grouped_values.append([value])

    return len(grouped_values)

def get_fuzzy_unique_counts(df, columns, threshold=85):
    return {col: get_fuzzy_unique_count(df[col], threshold) for col in columns}

column_names = [ 'reg', 'body_type', 'transmission', 'feul_type', 'model', 'make']
fuzzy_counts = get_fuzzy_unique_counts(df, column_names)

print(fuzzy_counts)

"""I used the fuzzy matching algorithm  to check for unique values that might not actually be unique, due to variations such as differences in letter case or missing characters.

The fuzz.ratio function from thefuzz library compares two strings based on their similarity using the Levenshtein distance, returning a score between 0 and 100. In the provided code, it groups similar values within a column by checking if their similarity score meets or exceeds the given threshold (85). This helps estimate the number of unique values while accounting for minor variations in spelling or formatting.

The results remained consistent across all features, with the exception of the "model" feature.

I will address the "model" feature separately.

**Reg:**

In some countries, vehicle registration plates include a code that indicates the year or period of registration. For example, in the UK, a car with a "T reg" plate was registered between August 1999 and July 2000, while an "X reg" plate corresponds to registrations between August 2000 and July 2001. These codes typically follow a structured pattern, allowing the registration year to be determined based on the "reg" value. By mapping these codes to specific years, they can be used for analysis.

However, this feature has a very high correlation with the "year" feature, which could lead to overfitting. Therefore, I am dropping this column.
"""

df = df.drop(columns=['reg'])

"""**Year:**

The "year" feature is categorical, but not entirely so. I will convert it into a numeric feature.

However, since there are 98 unique values, I find it hard to believe that cars being sold date back 98 years.
"""

unique_values = df['year'].unique()
unique_values

"""As we can see from the results, there are some unrelated unique values, such as "101 miles" and "Pickup."

I will proceed with converting the column to numeric. Any value that does not represent a valid year will be converted to NaN.

Afterward, I will handle the missing values by removing those rows.
"""

df['year'] = pd.to_numeric(df['year'], errors='coerce')

"""There are two unusual year values: 32 and 1079. First, I will check how many times they appear. If their occurrences are minimal, I will remove these rows, as they could negatively impact the analysis."""

count_32_1079 = df[df['year'].isin([32, 1079])].shape[0]
print(count_32_1079)

df = df[~df['year'].isin([32, 1079])]

"""Let's examine the rest of the categorical values."""

categorical_columns = df.select_dtypes(include=['object', 'category']).columns.tolist()
show_uniques_graph(categorical_columns)

"""**Filling Missing Data for Categorical Features**

Instead of simply dropping rows with missing values, I will group the data by the "make" feature (which represents the vehicle manufacturer). For each categorical feature, I will determine the most frequent value within each group and use it to fill in the missing data.
"""

# categorical missing data handled
cat_columns_with_missing = ['body_type', 'transmission', 'feul_type', 'model']
for col in cat_columns_with_missing:
    df[col] = df.groupby('make')[col].transform(
        lambda group: group.fillna(group.mode().iloc[0]) if not group.mode().empty else group
    )

"""After removing certain categories, we are left with two groups: categorical columns with a small number of unique values (except for the "model" feature).

I will apply one-hot encoding to these categorical columns.

For the "model" feature, I will use **hash encoding** to handle its high cardinality efficiently.

Hash encoding is a technique used to represent categorical variables with a large number of unique values by applying a hash function to map each category to a fixed-size vector. This reduces the dimensionality of the feature while maintaining uniqueness and preventing the model from being overwhelmed by the high cardinality. For a feature with around 1300 unique values, hash encoding is ideal because it allows for efficient storage and computation, especially when one-hot encoding would result in too many columns. This helps improve model performance and prevent memory issues while still capturing useful information from the feature.

# **Handling Binary features**
"""

binary_columns = ['ulez', 'full_service', 'part_service',
                    'full_dealership', 'first_year_road_tax', 'brand_new',
                    'finance_available', 'discounted', 'part_warranty']
target_column = 'car_price'
correlations = df[binary_columns + [target_column]].corr()[target_column].sort_values(ascending=False)

    # Filter out the target column itself
correlations = correlations.drop(target_column)

# Plotting
plt.figure(figsize=(10, 8))
sns.barplot(x=correlations.values, y=correlations.index, palette='coolwarm')
plt.title(f'Correlation between {", ".join(binary_columns)} and {target_column}')
plt.xlabel('Correlation Coefficient')
plt.ylabel('Features')
plt.show()

"""As we can see, the correlation with car price is not very high, but we have gained valuable insights from some of the binary features.

The low correlation does not necessarily mean these features are not useful for predicting the target variable. It is possible that, when combined with other features, they provide significant information.

For this reason, I will retain all of these features.

The only other step I would take is to convert these numerical features into categorical features, as they are currently represented numerically but may better serve the model as categories.
"""

boolean_features = ['ulez', 'full_service', 'part_service',
                    'full_dealership', 'first_year_road_tax', 'brand_new',
                    'finance_available', 'discounted', 'part_warranty']

df[boolean_features] = df[boolean_features].astype(bool)

"""# Handling Numeric Features

Let's examine the numeric features present in the dataset.
"""

numeric_columns = df.select_dtypes(include=['number']).columns.tolist()
print(numeric_columns)

"""As we observed earlier, the "miles" feature has a significant number of missing valuesâ€”around 6%.

To reduce this percentage, I will first check if the car is brand new. If it is, the "miles" value should be 0, so I will fill those missing cells with 0. For the remaining missing values, I will delete the rows, as the number of rows with null values is relatively small.

Before:
"""

missing_percentage = df['miles'].isna().mean() * 100
missing_percentage

"""After:"""

df.loc[df['brand_new'] == True, 'miles'] = df.loc[df['brand_new'] == True, 'miles'].fillna(0)
missing_percentage = df['miles'].isna().mean() * 100
missing_percentage

"""As we can see, the large number of missing values is due to the cars being new. I have updated those missing values to 0. Now, I will drop the rows that still have null values."""

df = df.dropna(subset=['miles'])

"""Lets handle first the remain of the missing values:"""

missing_percentages = df[['car_price', 'year', 'miles', 'engine_vol', 'engine_size']].isnull().mean() * 100
missing_percentages

"""To fill the missing values in the "year" feature, I will use the following approach:

1. For rows where the "brand_new" cell is true, I will fill the missing "year" value with the maximum year present in the dataset.

2. For other missing values, I will apply a clustering approach:

Create 10 clusters for the "miles" feature (e.g., 0-10%, 10-20%, etc.).
Calculate the mean "year" for each cluster.
Fill the missing "year" values based on the mean "year" of their respective "miles" cluster.
This approach should help fill in the missing year values in a way that preserves the relationship with other features.
"""

def fill_years(df_copy):
    max_year = df_copy['year'].max()
    df_copy.loc[df_copy['brand_new'] == True, 'year'] = max_year
    missing_percentages = df[['year']].isnull().mean() * 100
    print(f"missing precentage after filling the year value with max value: {missing_percentages}")

    num_bins = 10
    mile_min, mile_max = df_copy['miles'].min(), df_copy['miles'].max()
    bin_edges = np.linspace(mile_min, mile_max, num_bins)  # Create 10 equal-width bins

    df_copy['mile_bin'] = pd.cut(df_copy['miles'], bins=bin_edges, labels=False, include_lowest=True).astype(int) + 1
    mile_bin_medians = df_copy.dropna(subset=['year']).groupby('mile_bin')['year'].median().to_dict()

    def fill_missing_year(row):
        if pd.isna(row['year']):
            # Get the median year based on the mile_bin of the row
            median_year = mile_bin_medians.get(row['mile_bin'])
            if median_year:
                return median_year
        return row['year']

    df_copy['year'] = df_copy.apply(fill_missing_year, axis=1)
    df_copy['year'] = df_copy['year'].round().astype('Int64')  # Keeps NaNs
    df_copy.drop(columns=['mile_bin'], inplace=True)
fill_years(df)
missing_percentages = df[['year']].isnull().mean() * 100
print(f"missing precentage after filling the year value: {missing_percentages}")

"""**Correlation between features:**

As part of the pre-processing process, it is important to understand which features are highly correlated. If we observe strong correlations, we may consider removing one of the features to avoid multicollinearity. Additionally, we will look for features that have a significant impact on the target variable, which is the car price, to ensure the model's predictive power is optimized.
"""

selected_features = ['car_price', 'year', 'miles', 'engine_vol', 'engine_size']
corr_sample = df[selected_features].corr(numeric_only=True)

plt.figure(figsize=(7, 3))
sns.heatmap(corr_sample, annot=True, cmap='coolwarm', fmt=".2f", linewidths=0.5)
plt.title('Correlation Heatmap of Selected Features')
plt.show()

"""From the correlation matrix, we can observe that there is a high correlation (as expected) between the "miles," "year," "engine_size," "engine_vol," and the target featureâ€”car price. This suggests that these features are likely to have a significant impact on the car price, and we may want to focus on them during modeling."""

selected_features = ['car_price', 'year', 'miles', 'engine_vol', 'engine_size']
numeric_columns = df[selected_features]

correlation_with_car_price = numeric_columns.corr()['car_price'].sort_values(ascending=False)

plt.figure(figsize=(7, 3))
sns.barplot(x=correlation_with_car_price.values, y=correlation_with_car_price.index, palette='coolwarm')
plt.title('Selected Numerical Features Correlated with Car Price')
plt.xlabel('Correlation Coefficient')
plt.ylabel('Features')
plt.show()

"""From these plots, I can learn a lot. It's clear that the numerical features in our dataset have a significant influence on the target featureâ€”car price.

Another insight from the first plot is that "engine_size" and "engine_vol" have a very high correlation with each other. After reviewing the meanings of both features, I understand that they essentially convey the same information.

To avoid overfitting, the best approach would be to remove one of these features. I will drop the "engine_vol" feature, as it has a lower correlation with the car price feature.
"""

df = df.drop(columns=['engine_vol'])

"""CORRELATION - END

# **Handling outliers and Normal distribution:**
"""

plt.figure(figsize=(7, 3))
sns.kdeplot(df['car_price'], fill = True)
plt.xlabel('Car price')
plt.ylabel('Density')
plt.title('Distribution of car_price')
plt.xlim([-10000,300000])
plt.show()

"""As shown in the visual representation (the KDE plot, which reveals a long tail and numerous outliers in the higher price range) as well as the results from the DataFrame's describe function, the distribution of the "car_price" parameter does not follow a normal distribution.

This presents a challenge, as many straightforward models assume normality. To address this in the pre-processing step, we will handle the outliers by removing them and then re-evaluate the distribution to observe any changes in its characteristics.
"""

# Create a figure with subplots
numerical_col = list(df.describe().columns)
num_cols = len(numerical_col)
rows = (num_cols + 1) // 2
plt.figure(figsize=(12, rows * 4))

for i, col in enumerate(numerical_col, start=1):
    plt.subplot(rows, 2, i)
    sns.boxplot(data=df, x=col)
    plt.title(f'Boxplot of {col}', fontsize=12)
    plt.xlabel(col, fontsize=10)
    plt.ylabel('Values', fontsize=10)

# Adjust layout and show the plots
plt.tight_layout()
plt.show()

"""As shown in the boxplot distribution, it appears that there are significant issues with outliers in the data. I will proceed with further processing to handle these outliers and ensure the data is ready for modeling."""

def evaluate_outlier_removal(df, multipliers=[1.5, 2, 2.5, 3, 3.5, 4]):
    numerical_cols = df.select_dtypes(include=[np.number]).columns
    initial_rows = len(df)

    results = []

    for multiplier in multipliers:
        df_filtered = df.copy()

        for col in numerical_cols:
            Q1 = df_filtered[col].quantile(0.25)
            Q3 = df_filtered[col].quantile(0.75)
            IQR = Q3 - Q1
            lower_bound = Q1 - multiplier * IQR
            upper_bound = Q3 + multiplier * IQR
            df_filtered = df_filtered[(df_filtered[col] >= lower_bound) & (df_filtered[col] <= upper_bound)]

        final_rows = len(df_filtered)
        rows_dropped = initial_rows - final_rows
        percentage_dropped = (rows_dropped / initial_rows) * 100

        # Compute skewness & kurtosis before and after
        original_skewness = df[numerical_cols].apply(skew).mean()
        filtered_skewness = df_filtered[numerical_cols].apply(skew).mean()

        original_kurtosis = df[numerical_cols].apply(kurtosis).mean()
        filtered_kurtosis = df_filtered[numerical_cols].apply(kurtosis).mean()

        results.append({
            "Multiplier": multiplier,
            "Rows Dropped (%)": f"{percentage_dropped:.2f}%",
            "Skewness Before": f"{original_skewness:.2f}",
            "Skewness After": f"{filtered_skewness:.2f}",
            "Kurtosis Before": f"{original_kurtosis:.2f}",
            "Kurtosis After": f"{filtered_kurtosis:.2f}"
        })

    return pd.DataFrame(results)


result_df = evaluate_outlier_removal(df)
result_df

"""The multiplier has been adjusted to the data. Initially, a multiplier of 1.5 was chosen, which resulted in 15.03% of the data being deleted. To improve this, I have increased the multiplier to 4.0.

If the results are not satisfactory, I will apply a more refined filter to better handle the outliers.
"""

# Remove outliers
numerical_col = list(df.describe().columns)
df_original = df.copy()
multiplier = 4
for col in numerical_col:
    Q1 = df[col].quantile(0.25)
    Q3 = df[col].quantile(0.75)
    IQR = Q3 - Q1  # Interquartile Range
    lower_bound = Q1 - multiplier * IQR
    upper_bound = Q3 + multiplier * IQR
    df = df[(df[col] >= lower_bound) & (df[col] <= upper_bound)]

# Store the final number of rows
final_rows = len(df)
initial_rows = len(df_original)

# Calculate the percentage of rows dropped
rows_dropped = initial_rows - final_rows
percentage_dropped = (rows_dropped / initial_rows) * 100

# Print results
print(f"Initial rows: {initial_rows}")
print(f"Final rows: {final_rows}")
print(f"Rows dropped: {rows_dropped}")
print(f"Percentage of rows dropped: {percentage_dropped:.2f}%")

"""I plotted the boxplot again to check if, after removing the outliers, the distribution of car prices is more balanced and equal."""

num_cols = len(numerical_col)
rows = (num_cols + 1) // 2
plt.figure(figsize=(12, rows * 4))

for i, col in enumerate(numerical_col, start=1):
    plt.subplot(rows, 2, i)
    sns.boxplot(data=df, x=col)
    plt.title(f'Boxplot of {col}', fontsize=12)
    plt.xlabel(col, fontsize=10)
    plt.ylabel('Values', fontsize=10)

plt.tight_layout()

plt.figure(figsize=(7, 3))
sns.kdeplot(df['car_price'], fill = True)
plt.xlabel('Car price')
plt.ylabel('Density')
plt.title('Distribution of car_price')
plt.xlim([-10000,150000])
plt.show()

"""The process I followed here aims to minimize the skew while retaining as much data as possible.

The trade-off lies between deleting data and achieving a more normal distribution. While using a different multiplier could bring us closer to a normal distribution, the results so far look good with the current multiplier. I will adjust it further based on the results of the models.

# **Encoding the categorial data:**

I will encode all the categorical columns as follows:

**One-Hot Encoding:**

The features "body_type," "transmission," "fuel_type," and "make" will be encoded using one-hot encoding, creating separate columns for each category.

**Hash Encoding:**

The "model" feature will be encoded using hash encoding. Hash encoding transforms the categorical feature into a fixed number of new columns based on hash values. Each new column represents a portion of the hashed information, which reduces the risk of collisions. The number of columns is controlled by the n_features parameter.
To avoid excessive dimensionality while preserving accuracy, I will set n_features to at least log2(number of unique values) or higher. Since we have around 1300 unique values for the "model" feature, this means n_features should be approximately 11 or more (log2(1300) â‰ˆ 11).
"""

df = pd.get_dummies(df, columns=['body_type', 'transmission', 'feul_type', 'make'])
df

"""# **Part 2: Naive models**"""

!pip install category_encoders
import category_encoders as ce

# Define the number of hash bits (e.g., 10 will create 2^10 = 1024 features)
hash_encoder = ce.HashingEncoder(cols=['model'], n_components=10)

# Fit and transform
df_transformed = hash_encoder.fit_transform(df)

df_transformed

df = df_transformed

"""First, we will split the data into training and testing sets. Then, we will explore a few models that are suited for regression tasks: XGBoost, Gradient Boosting, KNN, and Linear Regression and Random Forest..

**Steps:**
Split the Data: Divide the data into training and testing sets to evaluate model performance.

Models to Test:

**XGBoost:** A powerful gradient boosting model known for its speed and performance.

**Gradient Boosting:** Another gradient boosting method, often providing robust results.

**Random Forest**: A robust ensemble method that uses multiple decision trees to reduce variance and overfitting.

**Linear Regression:** A simple but often effective model for regression tasks.

**Cross-Validation**: We will conduct 10-fold cross-validation to ensure the models generalize well on unseen data.

Data Normalization: Normalize the data within each fold to ensure proper scaling, as some models (like KNN) are sensitive to the scale of the data.

**Evaluation Metrics:**
Mean Squared Error (MSE): Measures the average squared difference between predicted and actual values.
RÂ²: Indicates how well the model explains the variance in the target variable.
We will run all models with the above steps and then evaluate their performance using MSE and RÂ² to determine the best-performing model.
"""

def train_model(model, x_train, y_train):
    model.fit(x_train, y_train)
    return model

def evaluate_model_kfold(model, x, y, k=10):
    kf = KFold(n_splits=k, shuffle=True, random_state=42)

    train_r2_scores, test_r2_scores = [], []
    train_mse_scores, test_mse_scores = [], []
    fold = 0
    for train_index, test_index in kf.split(x):
        x_train, x_valid = x.iloc[train_index], x.iloc[test_index]
        y_train, y_valid = y.iloc[train_index], y.iloc[test_index]

        scaler = StandardScaler()
        x_train_scaled = scaler.fit_transform(x_train)
        x_valid_scaled = scaler.transform(x_valid)

        trained_model = train_model(model, x_train_scaled, y_train)

        y_train_pred = trained_model.predict(x_train_scaled)
        y_valid_pred = trained_model.predict(x_valid_scaled)


        train_r2 = r2_score(y_train, y_train_pred)
        test_r2 = r2_score(y_valid, y_valid_pred)
        train_mse = mean_squared_error(y_train, y_train_pred)
        test_mse = mean_squared_error(y_valid, y_valid_pred)

        train_r2_scores.append(train_r2)
        test_r2_scores.append(test_r2)
        train_mse_scores.append(train_mse)
        test_mse_scores.append(test_mse)
        print(f"Fold {fold + 1}: Train R^2: {train_r2:.4f}, Test R^2: {test_r2:.4f}, Train MSE: {train_mse:.4f}, Test MSE: {test_mse:.4f}")
        fold += 1


    return {
        "Train R^2 AVG[SD]": f"{np.mean(train_r2_scores):.4f}[{np.std(train_r2_scores):.4f}]",
        "Test R^2 AVG[SD]": f"{np.mean(test_r2_scores):.4f}[{np.std(test_r2_scores):.4f}]",
        "Train MSE AVG[SD]": f"{np.mean(train_mse_scores):.4f}[{np.std(train_mse_scores):.4f}]",
        "Test MSE AVG[SD]": f"{np.mean(test_mse_scores):.4f}[{np.std(test_mse_scores):.4f}]"
    }

def evaluate_all_models(models, model_names, x, y, k=10):
    results = []
    for model_name, model in zip(model_names, models):
        print(f"Evaluating {model_name}...")
        scores = evaluate_model_kfold(model, x, y, k)
        scores["Model"] = model_name
        results.append(scores)
    return pd.DataFrame(results)

def split_data(df, goal_feature, columns=None):
  df_copy = df.copy()
  df_copy = df_copy.dropna(subset=[goal_feature])
  if columns is None:
      x = df_copy.drop(columns=[goal_feature])
  else:
      x = df_copy[columns]
  y = df_copy[goal_feature]
  return x,y

if not SKIP_FINFING_BEST_MODEL:
  model_names = ['XGBoost', 'Random Forest', 'Gradient Boosting', 'Linear Regression']
  models = [
      XGBRegressor(learning_rate=0.1, n_estimators=300, max_depth=5, random_state=101, reg_alpha=10, reg_lambda=10),
      RandomForestRegressor(n_estimators=50, max_depth=15, min_samples_split=5, min_samples_leaf=2, max_features='sqrt', bootstrap=True, n_jobs=-1, random_state=42),
      GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=5, random_state=42),
      LinearRegression()
  ]
  x, y = split_data(df, 'car_price')
  results_df = evaluate_all_models(models, model_names, x, y)
  print("\nFinal Results:")
  print(results_df)
else:
  with open("Final_result.txt", "r", encoding="utf-8") as file:
    content = file.read()
    print(content)

"""As we can see, I found a very good models that predict the car price in high precentage. I will focus on finding the best hyperparameters for XGBoost to improve its performance.

# **Optimizing Hyperparameters**
As seen before, my current best score achieved by XGBoost.

Let's try to improve that by optimizing the hyperparameters of our chosen model.

In this code, Iâ€™m tuning the hyperparameters of my XGBoost regression model to get the best performance. Iâ€™ve set up a grid of hyperparameters to test, like the number of estimators, learning rate, tree depth, and regularization terms. Iâ€™m using GridSearchCV to perform a cross-validation grid search, which tests all possible combinations of these parameters. It evaluates the models using 5-fold cross-validation, with the RÂ² score as the metric to assess performance. At the end, I print out the best combination of parameters and the highest RÂ² score to find the optimal setup for my model.
"""

if not SKIP_FINDING_HYPER_PARAMTER:
  # Define hyperparameter grid
  param_grid = {
      'n_estimators': [300,500,700],
      'learning_rate': [0.01, 0.1, 0.2],
      'max_depth': [5,9,11,13],
      'random_state': [101],
      'reg_alpha':[1, 5, 10],
      'reg_lambda': [10]

  }

  xgb = XGBRegressor(random_state=101)

  grid_search = GridSearchCV(estimator=xgb, param_grid=param_grid, cv=5, scoring='r2', verbose=2, n_jobs=-1)
  x, y = split_data(df, 'car_price')
  scaler = StandardScaler()
  x = scaler.fit_transform(x)
  grid_search.fit(x, y)

  print("Best parameters:", grid_search.best_params_)
  print("Best R^2 score:", grid_search.best_score_)
else:
    with open("Best_parameters_XGBRegressor.txt", "r", encoding="utf-8") as file:
      content = file.read()
      print(content)

"""This code is taking a long time to run.

To address this, I ran the grid search separately. I used it to find the best parameters that improved my model without overfitting.

The optimal parameters are:

XGBRegressor(
    learning_rate=0.1,
    n_estimators=500,
    max_depth=9,
    random_state=101,
    reg_alpha=10,
    reg_lambda=10
)
I will use these parameters to evaluate my model.

# **Num owner expirience**

The num_owner column represents the number of previous owners for each car. In real-world data, this feature significantly impacts car pricesâ€”generally, the more owners a car has had, the lower its price.

**Problem Statement**
Currently, 45% of the values in the num_owner column are missing. To address this, I plan to use the miles and year columns to predict the missing values.

**Approach**

I will experiment with:

**Train a regression model** (Linear Regression, XGBoost, or Gradient Boosting)

to predict num_owner based on available features.

**Evaluation Strategy**

After filling in the missing values using both methods, I will retrain my model and evaluate whether the prediction results improve.
One important factor affecting num_owner is the car price, but since it is the target variable, I cannot use it in the prediction processâ€”doing so would lead to **overfitting**.

By comparing both approaches, I aim to determine which method better improves the overall model performance.

**I saved the num_owners column at the begining of the colab.**

# **Regression**
"""

df_with_num_owner = df.copy()
df_with_num_owner = df_with_num_owner.join(num_owners_saved_df, how='left')
df_with_num_owner

if not SKIP_RUN_EXPIRIMENT:
  model_names = ['XGBoost', 'Linear Regression']
  models = [
      XGBRegressor(learning_rate=0.1, n_estimators=300, max_depth=5, random_state=101, reg_alpha=10, reg_lambda=10),
      LinearRegression()
  ]
  scaler = StandardScaler()
  x, y = split_data(df_with_num_owner, 'num_owner', ['year', 'miles'])

  # Run evaluation
  results_df = evaluate_all_models(models, model_names, x, y)
  print("\nFinal Results:")
  print(results_df)
else:
  with open("Expiriment_result.txt", "r", encoding="utf-8") as file:
      content = file.read()
      print(content)

"""The train and test RÂ² values are close, suggesting no major overfitting.
The test RÂ² (~0.3874) is relatively low, meaning the model is explaining only ~38.74% of the variance, indicating room for improvement.
The low standard deviations suggest that the model is performing consistently across different folds.

After I analyze the data and saw the result, I choosed the model: linear regression and now I will fill the num_owner column with the data that my model predict.

**Discussion on the Results**

As we can see from the results, the number of owners experiment did not succeed. The model was unable to accurately fill the num_owner column based solely on the miles and year columns.

The conclusion is to continue working with the original dataset rather than the num_owner_df, as the additional feature did not contribute meaningfully to the prediction task.

# **Model Performence Analysis**

After analyzing and optimizing our data and model, let's dive deeper and examine how our model behaves using SHAP (SHapley Additive exPlanations).

This method will help us understand the impact of each feature on the model's predictions, providing insights into the relative importance and effect of each variable within the model.
"""

X = df.drop(columns=['car_price'])
y = df['car_price']

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

X_train, X_test, y_train, y_test = [x.reset_index(drop=True) for x in [X_train, X_test, y_train, y_test]]

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Convert scaled training data into a DataFrame (for SHAP)
X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=X.columns)

model = XGBRegressor(
    learning_rate=0.1,
    n_estimators=500,
    max_depth=9,
    random_state=101,
    reg_alpha=10,
    reg_lambda=10
)

model.fit(X_train_scaled_df, y_train)
y_pred = model.predict(X_test_scaled)

shap_sample = X_train_scaled_df.sample(500, random_state=42)

explainer = shap.Explainer(model, X_train_scaled_df)
shap_values = explainer(shap_sample)

shap.plots.beeswarm(shap_values)

"""The x-axis represents the SHAP value (impact on model output). Positive values indicate a higher predicted price, while negative values indicate a lower predicted price.
The y-axis lists the features ranked by importance (top to bottom).
Each dot represents a data point, and its color indicates the feature value (red = high, blue = low).

**Year**

A higher year (newer cars, red) increases car price (positive SHAP values).
Older cars (blue) reduce the price (negative SHAP values).

**Engine Size**

Larger engines (red) increase the price.
Smaller engines (blue) decrease the price.

**Miles**

Higher mileage (red) decreases the price (negative SHAP values).
Low mileage (blue) increases the price (positive SHAP values).


**Transmission_Automatic**

Automatic transmission (red) generally increases the price.
Manual transmission (blue) tends to lower the price.

**Make_Land Rover**

Being a Land Rover (red) significantly increases the price.
Other makes (blue) reduce the price.

**Body Type - Hatchback**

Hatchback cars (red) tend to reduce the price.
Other body types (blue) might have a neutral or higher effect.

**Fuel Type - Petrol**

Petrol cars (red) slightly increase the price.
Other fuel types (blue) show a minor negative impact.

**Body Type - SUV**

SUVs (red) increase the price.
Other body types (blue) have a neutral or lower impact.

**Make - Mercedes-Benz**

Being a Mercedes-Benz (red) increases the price significantly.
Other brands (blue) have a lower price impact.

**Sum of 106 other features**

These features have less impact individually but collectively influence the price prediction.

for conclusion, Newer cars, SUVs, and premium brands (Land Rover, Mercedes-Benz) increase price.
Higher mileage, older cars, and hatchbacks lower price.
Transmission type and fuel type have moderate effects.

I did not expect engenine size to impact so much on the model. I thought it will be, the more the miles and the type of the car - luxury cars as mercedes and BMW.
"""

sample_size = 10000  # Adjust based on performance needs
X_sample = X_train_scaled_df.sample(sample_size, random_state=42)

explainer = shap.TreeExplainer(model)
shap_values = explainer.shap_values(X_sample)

shap.dependence_plot("engine_size", shap_values, X_sample)

"""This SHAP dependence plot illustrates how engine_size influences the model's predictions. As engine_size increases, SHAP values also rise, indicating that larger engines contribute to higher predicted values. The color gradient represents the year feature, showing its interaction with engine_size, where the transition from blue to red suggests that the impact of engine size varies depending on the year. Additionally, there is noticeable variance in SHAP values for larger engines, implying that other factors also influence predictions. The increasing spread of SHAP values at higher engine sizes further highlights this variability. This trend is reasonable because newer vehicles (higher year values) often have larger engines due to technological advancements, evolving consumer preferences, and regulatory changes that impact engine design and performance.

# **Error Analysis**
"""

pred_diff = y_pred - y_test

max_positive_diff_index = np.argmax(pred_diff)

max_positive_diff_car = X_test.iloc[max_positive_diff_index]

# Find the columns with True values in the closest_diff_car row
true_columns = max_positive_diff_car[max_positive_diff_car == True].index.tolist()

# Print the names of the columns with True values
print("Columns with True values:")
print(true_columns)


real_price = y_test.iloc[max_positive_diff_index]
predicted_price = y_pred[max_positive_diff_index]
difference = pred_diff[max_positive_diff_index]

print("Car details with the maximum positive price difference:\n", max_positive_diff_car)
print(f"Real Price: {real_price}")
print(f"Predicted Price: {predicted_price}")
print(f"Difference: {difference}")



miles = max_positive_diff_car['miles']
year = max_positive_diff_car['year']
engine_size = max_positive_diff_car['engine_size']

print(f"Miles: {miles}")
print(f"Year: {year}")
print(f"Engine Size: {engine_size}")

pos_car_idx = X_test.index[max_positive_diff_index]

X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=X_test.columns)

single_car = X_test_scaled_df.iloc[[max_positive_diff_index]]

explainer = shap.TreeExplainer(model)
shap_values_single = explainer(single_car)

shap.plots.bar(shap_values_single, max_display=12)

"""**Worse prediction example:**

The significant difference between the predicted price (80050.25) and the actual price (29662.25) suggests that the model has overestimated the car's value by approximately 29,662.25.

The data presented indicates that the car is a large van powered by a diesel engine, specifically a Volkswagen, with no mileage information. It has a relatively large engine and overall good specifications.

The model likely overestimated the price due to several high-value features influencing the prediction. These include the car being a Volkswagen, which is an expensive brand, having a relatively large engine that further increases the price, and being a new car, which also drives the price up.

This car likely represents an outlier case, as it's difficult to estimate the prices of van-type vehicles. Out of the 800,000 rows in our dataset, only 400 are of the van type.

Therefore, the model probably overestimated the value of this vehicle due to the lack of sufficient data for this category of car.

# **Best predicted example:**
"""

pred_diff = y_pred - y_test

closest_diff_index = np.argmin(np.abs(pred_diff))

closest_diff_car = X_test.iloc[closest_diff_index]

# Find the columns with True values in the closest_diff_car row
true_columns = closest_diff_car[closest_diff_car == True].index.tolist()

# Print the names of the columns with True values
print("Columns with True values:")
print(true_columns)

real_price = y_test.iloc[closest_diff_index]
predicted_price = y_pred[closest_diff_index]
difference = pred_diff[closest_diff_index]

print("Car details with the closest predicted price:\n", closest_diff_car)
print(f"Real Price: {real_price}")
print(f"Predicted Price: {predicted_price}")
print(f"Difference: {difference}")

miles = closest_diff_car['miles']
year = closest_diff_car['year']
engine_size = closest_diff_car['engine_size']

print(f"Miles: {miles}")
print(f"Year: {year}")
print(f"Engine Size: {engine_size}")

pos_car_idx = X_test.index[closest_diff_index]

X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=X_test.columns)

single_car = X_test_scaled_df.iloc[[closest_diff_index]]

explainer = shap.TreeExplainer(model)
shap_values_single = explainer(single_car)

shap.plots.bar(shap_values_single, max_display=12)

"""The make of the car is Mini.

Miles: 38,000.0

Year: 2013

Engine Size: 122.0

As I mentioned earlier, engine size doesn't have much impact in this case because the engine is small. Additionally, the car's year doesn't affect the price much either, as the car is 9 years old.

Mini cars are generally inexpensive, so given these features, the model predicts a low price, which aligns with the actual car price.

![Q5_2-M.jpg](data:image/jpeg;base64,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)

# **Audi example:**
"""

volvo_high_price = X_test[(df['make_Audi'] == True) & (y_test > 20000)]

single_volvo = volvo_high_price.iloc[0]

real_price_volvo = y_test.loc[single_volvo.name]
predicted_price_volvo = y_pred[single_volvo.name]
difference_volvo = predicted_price_volvo - real_price_volvo

print("Car details with 'make_Volvo' = True and price > 20,000:")
print(single_volvo)
print(f"Real Price: {real_price_volvo}")
print(f"Predicted Price: {predicted_price_volvo}")
print(f"Difference: {difference_volvo}")

miles = single_volvo['miles']
year = single_volvo['year']
engine_size = single_volvo['engine_size']

print(f"Miles: {miles}")
print(f"Year: {year}")
print(f"Engine Size: {engine_size}")

X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=X_test.columns)

single_car_volvo_scaled = X_test_scaled_df.loc[single_volvo.name].values.reshape(1, -1)

explainer = shap.TreeExplainer(model)
shap_values_single = explainer.shap_values(single_car_volvo_scaled)

shap_values_single_explanation = shap.Explanation(values=shap_values_single,
                                                  base_values=explainer.expected_value,
                                                  data=single_car_volvo_scaled,
                                                  feature_names=X_test.columns)

shap.plots.bar(shap_values_single_explanation, max_display=12)

"""I randomly selected an Audi car with a price greater than 20,000.

The Audi I chose has the following data:

Miles: 0.0

Year: 2022

Engine Size: 190.0

From this data, I can infer that the Audi has a relatively small engine size, but it is a new car. The model's predicted price difference was only -2,222.48.

As shown in the SHAP graph, the model primarily predicts the price based on the car being brand new with 0 miles.

As I mentioned earlier, engine size plays a significant role in price prediction â€” the larger the engine size, the higher the price. However, in this case, the small engine size doesn't have as much of an impact on the car's price, likely because the car is so new.

## **Code Notebook from Kaggle**

I searched for similar projects using this dataset but didnâ€™t find anything.
I will probably upload my notebook to Kaggle to get feedback and commen

In this project, I worked with a dataset of used cars in England.
I intentionally chose this dataset knowing that it had challenges so that I could tackle them effectively and make the project more meaningful.

After numerous experiments with the data, I realized that in order to use it effectively for predicting the price of a used car based on its features, I needed to process and transform the dataset to achieve a high-accuracy prediction.

One of the challenges I encountered was that the data was not normally distributed. To improve the predictions, I worked on transforming the dataset to follow a more normal distribution.

To improve model performance, I categorized the features into binary, categorical, and numerical data.
For each type, I applied different processing techniques to ensure the dataset was structured optimally for training.

The dataset had missing values, which I handled by filling in data where possible, removing rows with excessive missing values, and analyzing the data for consistency.
I also identified and addressed numerous outliers that were distorting the analysis.
Many features had little to no correlation with the target feature, so I performed feature selection to remove irrelevant columns and kept only those valuable for predicting car prices.

These data cleaning steps were essential for optimizing the dataset for accurate predictions.

Once the dataset was properly prepared, I tested multiple machine learning algorithms to determine which model would provide the best price predictions without overfitting.

The XGBoost model achieved the best results. To further enhance its performance, I applied Grid Search to find the optimal hyperparameters.

To better understand how the model made predictions, I used SHAP values.
I visualized these values in different ways to interpret and explain how each feature influenced the predicted price.

**Conclusion**

In this project, my goal was to achieve the most accurate possible car price predictions by processing the dataset effectively, choosing the right model, and optimizing its performance.
"""